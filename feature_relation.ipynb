{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6efc330f-f227-4865-a655-4971b5f84c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "covid_data = pd.read_csv('dataset/modelling_data/cases/covid-case-counts.csv',low_memory=False)\n",
    "weather_data = pd.read_csv('dataset/modelling_data/niwa/weather_data.csv',low_memory=False)\n",
    "\n",
    "# Convert the 'Report Date' and 'Date' columns to datetime\n",
    "covid_data['Report Date'] = pd.to_datetime(covid_data['Report Date'])\n",
    "weather_data['Date'] = pd.to_datetime(weather_data['Date'])\n",
    "\n",
    "# Align the datasets to the same time range\n",
    "start_date = max(covid_data['Report Date'].min(), weather_data['Date'].min())\n",
    "end_date = min(covid_data['Report Date'].max(), weather_data['Date'].max())\n",
    "\n",
    "covid_data = covid_data[(covid_data['Report Date'] >= start_date) & (covid_data['Report Date'] <= end_date)]\n",
    "weather_data = weather_data[(weather_data['Date'] >= start_date) & (weather_data['Date'] <= end_date)]\n",
    "\n",
    "# Merge the datasets on the date columns\n",
    "merged_data = pd.merge(covid_data, weather_data, left_on='Report Date', right_on='Date')\n",
    "\n",
    "# Drop the redundant date column\n",
    "merged_data = merged_data.drop(columns=['Date'])\n",
    "\n",
    "merged_data.to_csv('dataset/preprocessed/preprocessed_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71909ca5-c338-4247-9ad2-ece4337ded93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Define the data types for each column to save memory\n",
    "dtypes = {\n",
    "    'Report Date': 'str',\n",
    "    'Number of cases reported': 'float32',\n",
    "    'WSpd.m.s.': 'float32',\n",
    "    'WindRun.Km.': 'float32',\n",
    "    'Tmax.C.': 'float32',\n",
    "    'Tmin.C.': 'float32'\n",
    "}\n",
    "\n",
    "# Load and process the data in chunks\n",
    "chunksize = 10 ** 6\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv('dataset/preprocessed/preprocessed_data.csv', dtype=dtypes, usecols=list(dtypes.keys()), chunksize=chunksize):\n",
    "    # Group by 'Report Date' and sum 'Number of cases reported'\n",
    "    chunk = chunk.groupby('Report Date', as_index=False).agg({\n",
    "        'Number of cases reported': 'sum',\n",
    "        'WSpd.m.s.': 'mean',\n",
    "        'WindRun.Km.': 'mean',\n",
    "        'Tmax.C.': 'mean',\n",
    "        'Tmin.C.': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Handle missing values (if any)\n",
    "    chunk = chunk.fillna(chunk.mean())\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    chunk[list(dtypes.keys())[1:]] = scaler.fit_transform(chunk[list(dtypes.keys())[1:]])\n",
    "    \n",
    "    chunks.append(chunk)\n",
    "\n",
    "# Concatenate all chunks\n",
    "df = pd.concat(chunks)\n",
    "\n",
    "df = df.drop(columns=['Report Date'])\n",
    "# Save the processed data\n",
    "df.to_csv('dataset/processed_data.csv', index=False,header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff5792be-da20-437c-89cd-0da577194ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "covid_data = pd.read_csv('dataset/modelling_data/cases/covid-case-counts.csv',low_memory=False)\n",
    "weather_data = pd.read_csv('dataset/modelling_data/niwa/weather_data.csv',low_memory=False)\n",
    "crime_data = pd.read_csv('dataset/modelling_data/nz_crime/nz_crime.csv',low_memory=False)\n",
    "\n",
    "# Convert the 'Report Date', 'Date', and 'Year Month' columns to datetime\n",
    "covid_data['Report Date'] = pd.to_datetime(covid_data['Report Date'])\n",
    "weather_data['Date'] = pd.to_datetime(weather_data['Date'])\n",
    "crime_data['Year Month'] = pd.to_datetime(crime_data['Year Month'])\n",
    "\n",
    "# Resample the covid data and the weather data to a monthly frequency\n",
    "covid_data = covid_data.resample('M', on='Report Date').sum()\n",
    "covid_data.index = covid_data.index + pd.offsets.MonthBegin(-1)\n",
    "weather_data = weather_data.resample('M', on='Date').mean()\n",
    "weather_data.index = weather_data.index + pd.offsets.MonthBegin(-1)\n",
    "\n",
    "start_date = max(covid_data.index.min(), weather_data.index.min(), crime_data['Year Month'].min())\n",
    "end_date = min(covid_data.index.max(), weather_data.index.max(), crime_data['Year Month'].max())\n",
    "\n",
    "covid_data = covid_data[(covid_data.index >= start_date) & (covid_data.index <= end_date)]\n",
    "weather_data = weather_data[(weather_data.index >= start_date) & (weather_data.index <= end_date)]\n",
    "crime_data = crime_data[(crime_data['Year Month'] >= start_date) & (crime_data['Year Month'] <= end_date)]\n",
    "\n",
    "\n",
    "#print(covid_data.head(n=100))\n",
    "#print(weather_data.head(n=100))\n",
    "#print(crime_data.head(n=100))\n",
    "\n",
    "\n",
    "# Merge the datasets on the date columns\n",
    "merged_data = pd.merge(covid_data, weather_data, left_index=True, right_index=True)\n",
    "#print(merged_data.head(n=100))\n",
    "merged_data = pd.merge(merged_data, crime_data, left_index=True, right_on='Year Month')\n",
    "#print(merged_data.head(n=100))\n",
    "# Drop the redundant date column\n",
    "#merged_data = merged_data.drop(columns=['Year Month'])\n",
    "#print(merged_data.head(n=100))\n",
    "# Preprocess the data for StemGNN\n",
    "# This step will depend on the specific requirements of the StemGNN model\n",
    "\n",
    "merged_data.to_csv('dataset/preprocessed/preprocessed_data1.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec81b0df-05e4-4c14-86b9-b1faecce8a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Define the data types for each column to save memory\n",
    "dtypes = {\n",
    "    'Year Month': 'str',\n",
    "    'Number of cases reported': 'float32',\n",
    "    'WSpd.m.s.': 'float32',\n",
    "    'WindRun.Km.': 'float32',\n",
    "    'Tmax.C.': 'float32',\n",
    "    'Tmin.C.': 'float32',\n",
    "    'vict_sum': 'float32',\n",
    "    'vict_cnt': 'float32'\n",
    "}\n",
    "\n",
    "# Load and process the data in chunks\n",
    "chunksize = 10 ** 6\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv('dataset/preprocessed/preprocessed_data1.csv', dtype=dtypes, usecols=list(dtypes.keys()), chunksize=chunksize):\n",
    "    # Group by 'Report Date' and sum 'Number of cases reported'\n",
    "    chunk = chunk.groupby('Year Month', as_index=False).agg({\n",
    "        'Number of cases reported': 'sum',\n",
    "        'WSpd.m.s.': 'mean',\n",
    "        'WindRun.Km.': 'mean',\n",
    "        'Tmax.C.': 'mean',\n",
    "        'Tmin.C.': 'mean',\n",
    "        'vict_sum': 'sum',\n",
    "        'vict_cnt': 'sum'\n",
    "    })\n",
    "    \n",
    "    # Handle missing values (if any)\n",
    "    chunk = chunk.fillna(chunk.mean())\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    chunk[list(dtypes.keys())[1:]] = scaler.fit_transform(chunk[list(dtypes.keys())[1:]])\n",
    "    \n",
    "    chunks.append(chunk)\n",
    "\n",
    "# Concatenate all chunks\n",
    "df = pd.concat(chunks)\n",
    "\n",
    "df = df.drop(columns=['Year Month'])\n",
    "# Save the processed data\n",
    "df.to_csv('dataset/processed_data1.csv', index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33c3df17-cf84-4fd8-8446-6d62c529ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "def load_and_resample(path, date_column, columns, start_date=None, end_date=None, frequency='D'):\n",
    "    df = pd.read_csv(path,low_memory=False)\n",
    "    df[date_column] = pd.to_datetime(df[date_column])  # convert date column to datetime\n",
    "    df = df[columns + [date_column]]  # select only the specific columns and date column\n",
    "    df = df.groupby(date_column).sum().reset_index()  # sum values on duplicate dates\n",
    "    df.set_index(date_column, inplace=True)  # set date column as index\n",
    "    df_resampled = df.resample(frequency).sum()  # resample data to daily frequency\n",
    "    #df_resampled.fillna(0, inplace=True) # fill missing values with 0\n",
    "    if start_date and end_date:\n",
    "        all_dates = pd.date_range(start=start_date, end=end_date, freq=frequency)\n",
    "        df_resampled = df_resampled.reindex(all_dates, fill_value=0)  # trim date range\n",
    "    return df_resampled\n",
    "\n",
    "def normalize_data(df, path_to_save_scaler):\n",
    "    scalers = []\n",
    "    for i in range(df.shape[1]):\n",
    "        scaler = MinMaxScaler()\n",
    "        df.iloc[:, i] = scaler.fit_transform(df.iloc[:, i].values.reshape(-1, 1))\n",
    "        scalers.append(scaler)  # store the fitted scaler for later use\n",
    "\n",
    "    # Save the scalers\n",
    "    joblib.dump(scalers, path_to_save_scaler) \n",
    "    return df\n",
    "\n",
    "def preprocess_data(paths, date_columns, columns_list, output_path):\n",
    "    \n",
    "    # Load covid-case-counts data and find its date range\n",
    "    covid_data = load_and_resample(paths[0], date_columns[0], columns_list[0])\n",
    "    #covid_data = normalize_data(covid_data,f'scaler_0.pkl')\n",
    "    start_date = covid_data.index.min()\n",
    "    end_date = covid_data.index.max()\n",
    "\n",
    "    # Preprocess all other datasets\n",
    "    preprocessed_data = []\n",
    "    for i, (path, date_column, columns) in enumerate(zip(paths, date_columns, columns_list)):\n",
    "        df = load_and_resample(path, date_column, columns, start_date, end_date)\n",
    "        df = normalize_data(df, f'scaler_{i}.pkl')  # normalize data\n",
    "        preprocessed_data.append(df)\n",
    "\n",
    "    # Concatenate all dataframes along the columns axis\n",
    "    merged_data = pd.concat(preprocessed_data, axis=1)\n",
    "\n",
    "    # Save merged data to CSV\n",
    "    merged_data.reset_index(drop=True, inplace=True)\n",
    "    merged_data.to_csv(output_path,index=False,header=False)\n",
    "\n",
    "\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "paths = ['dataset/modelling_data/cases/covid-case-counts.csv', 'dataset/modelling_data/niwa/weather_data.csv', 'dataset/modelling_data/nz_crime/nz_crime.csv', 'dataset/modelling_data/cases/weekly-hospitalisations-for-covid.csv', 'dataset/modelling_data/statsnz/regional/CPACT15_electricity.csv', 'dataset/modelling_data/statsnz/regional/CPEMP8_jobs.csv']\n",
    "date_columns = ['Report Date', 'Date', 'Year Month', 'Admissions for COVID-19 in the week ending', 'Period', 'Period']\n",
    "columns_list = [['Number of cases reported'], ['WSpd.m.s.', 'WindRun.Km.', 'Tmax.C.', 'Tmin.C.'], ['vict_sum', 'vict_cnt'], ['Hospitalisations'], ['Value'], ['Value']]\n",
    "output_path = 'dataset/all_preprocessed_data.csv'\n",
    "\n",
    "# Preprocess all datasets and merge them\n",
    "merged_data = preprocess_data(paths, date_columns, columns_list, output_path)\n",
    "# Now 'merged_data' is a dataframe that contains all preprocessed data, and it is also saved to a CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc502e28-0700-4732-b788-ac4c8fae7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['dataset/modelling_data/cases/covid-case-counts.csv', 'dataset/modelling_data/niwa/weather_data.csv', 'dataset/modelling_data/nz_crime/nz_crime.csv', 'dataset/modelling_data/cases/weekly-hospitalisations-for-covid.csv', 'dataset/modelling_data/statsnz/regional/CPACT15_electricity.csv', 'dataset/modelling_data/statsnz/regional/CPEMP8_jobs.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c182c43-b139-4c5f-bef4-5c458eec9568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Report Date', 'At the border', 'Auckland', 'Bay of Plenty',\n",
      "       'Canterbury/West Coast', 'Capital & Coast/Hutt', 'Counties Manukau',\n",
      "       'Hawke's Bay', 'Lakes', 'MidCentral', 'Nelson Marlborough', 'Northland',\n",
      "       'South Canterbury', 'Southern', 'Tairawhiti', 'Taranaki', 'Unknown',\n",
      "       'Waikato', 'Wairarapa', 'Waitemata', 'Whanganui'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Read the data from the CSV file\n",
    "data = pd.read_csv('dataset/covid-case-counts-processed-plot.csv', low_memory=False)\n",
    "head_names = data.head().columns\n",
    "print(head_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce74c8e-32fa-4475-bf77-68ac9ae227bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "def load_and_resample(path, date_column, columns, start_date=None, end_date=None, frequency='D'):\n",
    "    df = pd.read_csv(path,low_memory=False)\n",
    "    df[date_column] = pd.to_datetime(df[date_column])  # convert date column to datetime\n",
    "    df = df[columns + [date_column]]  # select only the specific columns and date column\n",
    "    df = df.groupby(date_column).sum().reset_index()  # sum values on duplicate dates\n",
    "    df.set_index(date_column, inplace=True)  # set date column as index\n",
    "    df.fillna(0, inplace=True) # fill missing values with 0\n",
    "    if start_date and end_date:\n",
    "        all_dates = pd.date_range(start=start_date, end=end_date, freq=frequency)\n",
    "        df = df.reindex(all_dates, fill_value=0)  # trim date range\n",
    "    return df\n",
    "\n",
    "def normalize_data(df, path_to_save_scaler):\n",
    "    scalers = []\n",
    "    for i in range(df.shape[1]):\n",
    "        scaler = MinMaxScaler()\n",
    "        df.iloc[:, i] = scaler.fit_transform(df.iloc[:, i].values.reshape(-1, 1))\n",
    "        scalers.append(scaler)  # store the fitted scaler for later use\n",
    "\n",
    "    # Save the scalers\n",
    "    joblib.dump(scalers, path_to_save_scaler) \n",
    "    return df\n",
    "\n",
    "def preprocess_data(paths, date_columns, columns_list, output_path):\n",
    "    \n",
    "    # Load covid-case-counts data and find its date range\n",
    "    covid_data = load_and_resample(paths[0], date_columns[0], columns_list[0])\n",
    "    covid_data = normalize_data(covid_data,f'1scaler_0.pkl')\n",
    "    start_date = covid_data.index.min()\n",
    "    end_date = covid_data.index.max()\n",
    "\n",
    "    # Preprocess all other datasets\n",
    "    preprocessed_data = [covid_data]\n",
    "    df = load_and_resample(paths[1], date_columns[1], columns_list[1], start_date, end_date)\n",
    "    df = normalize_data(df, f'1scaler_{1}.pkl')  # normalize data\n",
    "    preprocessed_data.append(df)\n",
    "    \n",
    "\n",
    "    # Concatenate all dataframes along the columns axis\n",
    "    merged_data = pd.concat(preprocessed_data, axis=1)\n",
    "\n",
    "    # Save merged data to CSV\n",
    "    merged_data.reset_index(drop=True, inplace=True)\n",
    "    merged_data.fillna(0, inplace=True)\n",
    "    merged_data.to_csv(output_path,index=False,header=False)\n",
    "\n",
    "\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "paths = ['dataset/modelling_data/cases/covid-case-counts.csv', 'dataset/modelling_data/niwa/weather_data.csv']\n",
    "date_columns = ['Report Date', 'Date']\n",
    "columns_list = [['Number of cases reported'], ['WSpd.m.s.', 'WindRun.Km.', 'Tmax.C.', 'Tmin.C.']]\n",
    "output_path = 'dataset/preprocessed_data.csv'\n",
    "\n",
    "# Preprocess all datasets and merge them\n",
    "merged_data = preprocess_data(paths, date_columns, columns_list, output_path)\n",
    "# Now 'merged_data' is a dataframe that contains all preprocessed data, and it is also saved to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9392229e-e609-4a36-99f0-cbef1983cff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
